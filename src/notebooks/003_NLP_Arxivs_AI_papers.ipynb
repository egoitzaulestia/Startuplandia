{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP en artículos científiocs (papers) de Arxiv sobre Inteligencia Artificial hata 2022**\n",
    "En este notebook se trabaja el procesamiento de lenguaje natura (NLP) mediante la busqueda de bigramas y trigramas en los textos de los titulos y los abstracs del dataset de artículos científicos de Arxiv sobre inteligencia artifical, con la intención de hayar información de en que campos o conceptos de la inteligencia artifical se esta trabajaron más. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/egoitzaulestiapadilla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/egoitzaulestiapadilla/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "arxivs_papers = \"../data/arxiv_papers.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and columns: (8000, 5) \n",
      "\n",
      "Tamaño del data frame: 40000\n",
      "\n",
      "Data frame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>published</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MetaFormer is Actually What You Need for Vision</td>\n",
       "      <td>Transformers have shown great potential in com...</td>\n",
       "      <td>2021-11-22T18:52:03+00:00</td>\n",
       "      <td>['Weihao Yu', 'Mi Luo', 'Pan Zhou', 'Chenyang ...</td>\n",
       "      <td>http://arxiv.org/abs/2111.11418v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Turbo Autoencoder with a Trainable Interleaver</td>\n",
       "      <td>A critical aspect of reliable communication in...</td>\n",
       "      <td>2021-11-22T18:37:03+00:00</td>\n",
       "      <td>['Karl Chahine', 'Yihan Jiang', 'Pooja Nuti', ...</td>\n",
       "      <td>http://arxiv.org/abs/2111.11410v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ab-initio calculation of point defect equilibr...</td>\n",
       "      <td>Point defects are responsible for a wide range...</td>\n",
       "      <td>2021-11-22T17:11:17+00:00</td>\n",
       "      <td>['Mubashir Mansoor', 'Mehya Mansoor', 'Maryam ...</td>\n",
       "      <td>http://arxiv.org/abs/2111.11359v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Divergent electrostriction at ferroelectric ph...</td>\n",
       "      <td>We investigate the electrostrictive response a...</td>\n",
       "      <td>2021-11-22T17:00:32+00:00</td>\n",
       "      <td>['Daniel S. P. Tanner', 'Pierre-Eymeric Janoli...</td>\n",
       "      <td>http://arxiv.org/abs/2111.11352v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ProxyFL: Decentralized Federated Learning thro...</td>\n",
       "      <td>Institutions in highly regulated domains such ...</td>\n",
       "      <td>2021-11-22T16:47:39+00:00</td>\n",
       "      <td>['Shivam Kalra', 'Junfeng Wen', 'Jesse C. Cres...</td>\n",
       "      <td>http://arxiv.org/abs/2111.11343v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    MetaFormer is Actually What You Need for Vision   \n",
       "1     Turbo Autoencoder with a Trainable Interleaver   \n",
       "2  Ab-initio calculation of point defect equilibr...   \n",
       "3  Divergent electrostriction at ferroelectric ph...   \n",
       "4  ProxyFL: Decentralized Federated Learning thro...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Transformers have shown great potential in com...   \n",
       "1  A critical aspect of reliable communication in...   \n",
       "2  Point defects are responsible for a wide range...   \n",
       "3  We investigate the electrostrictive response a...   \n",
       "4  Institutions in highly regulated domains such ...   \n",
       "\n",
       "                   published  \\\n",
       "0  2021-11-22T18:52:03+00:00   \n",
       "1  2021-11-22T18:37:03+00:00   \n",
       "2  2021-11-22T17:11:17+00:00   \n",
       "3  2021-11-22T17:00:32+00:00   \n",
       "4  2021-11-22T16:47:39+00:00   \n",
       "\n",
       "                                             authors  \\\n",
       "0  ['Weihao Yu', 'Mi Luo', 'Pan Zhou', 'Chenyang ...   \n",
       "1  ['Karl Chahine', 'Yihan Jiang', 'Pooja Nuti', ...   \n",
       "2  ['Mubashir Mansoor', 'Mehya Mansoor', 'Maryam ...   \n",
       "3  ['Daniel S. P. Tanner', 'Pierre-Eymeric Janoli...   \n",
       "4  ['Shivam Kalra', 'Junfeng Wen', 'Jesse C. Cres...   \n",
       "\n",
       "                                 url  \n",
       "0  http://arxiv.org/abs/2111.11418v1  \n",
       "1  http://arxiv.org/abs/2111.11410v1  \n",
       "2  http://arxiv.org/abs/2111.11359v1  \n",
       "3  http://arxiv.org/abs/2111.11352v1  \n",
       "4  http://arxiv.org/abs/2111.11343v1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el dataset, creamos e imprimimos información de filas, columnas, tamaño y el data frame\n",
    "df_arxivs_papers = pd.read_csv(arxivs_papers, encoding=\"ISO-8859-1\")\n",
    "print(f\"Rows and columns: {df_arxivs_papers.shape}\", \"\\n\")\n",
    "print(f\"Tamaño del data frame: {df_arxivs_papers.size}\\n\")\n",
    "print(\"Data frame:\")\n",
    "df_arxivs_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      8000 non-null   object\n",
      " 1   abstract   8000 non-null   object\n",
      " 2   published  8000 non-null   object\n",
      " 3   authors    8000 non-null   object\n",
      " 4   url        8000 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_arxivs_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 20 palabras más repetidas en los títulos del dataset\n",
      "de artículos cintíficos de Arxiv sobre Inteligencia Artificial:\n",
      "---------------------------------------------------------------\n",
      "the: 87429\n",
      "of: 49288\n",
      "and: 43112\n",
      "to: 37277\n",
      "a: 35083\n",
      "in: 27711\n",
      "we: 19752\n",
      "for: 17842\n",
      "is: 17017\n",
      "that: 13599\n",
      "with: 12633\n",
      "on: 12121\n",
      "this: 10938\n",
      "by: 8782\n",
      "are: 8665\n",
      "as: 8406\n",
      "an: 7765\n",
      "our: 6364\n",
      "can: 6111\n",
      "design: 6020\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de Datos\n",
    "titulos = df_arxivs_papers['abstract'].str.lower().str.replace(r'[^\\w\\s]', '').str.split()\n",
    "\n",
    "# Aplanamos la lista de palabras\n",
    "todas_las_palabras = [palabra for titulo in titulos for palabra in titulo]\n",
    "\n",
    "# Contamos la frecuencia de palabras\n",
    "frecuencia_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Encontramos las palabras más comunes\n",
    "palabras_mas_comunes = frecuencia_palabras.most_common(20)\n",
    "\n",
    "print(\"Las 20 palabras más repetidas en los títulos del dataset\\\n",
    "\\nde artículos cintíficos de Arxiv sobre Inteligencia Artificial:\\\n",
    "\\n---------------------------------------------------------------\")\n",
    "# Imprimimos las palabras más comunes\n",
    "for palabra, cuenta in palabras_mas_comunes:\n",
    "    print(f'{palabra}: {cuenta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que esas palabras no nos dicen nada. Vamos importar \"stopwords\" para filtrar las palabras que no nos aporten nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 20 palabras más repetidas en los títulos del dataset\n",
      "de artículos cintíficos de Arxiv sobre Inteligencia Artificial:\n",
      "---------------------------------------------------------------\n",
      "learning: 977\n",
      "design: 617\n",
      "using: 468\n",
      "neural: 411\n",
      "networks: 383\n",
      "network: 372\n",
      "deep: 371\n",
      "data: 310\n",
      "via: 310\n",
      "based: 308\n",
      "systems: 307\n",
      "control: 292\n",
      "detection: 256\n",
      "model: 240\n",
      "quantum: 237\n",
      "optimization: 232\n",
      "approach: 230\n",
      "graph: 206\n",
      "analysis: 205\n",
      "system: 202\n",
      "---------------------------------------------------------------\n",
      "Tiempo de busqueda transcurrido: 0.13 segundos\n"
     ]
    }
   ],
   "source": [
    "# Registramos tiempo de inicio\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# Preprocesamiento de Datos\n",
    "titulos = df_arxivs_papers['title'].str.lower().str.replace(r'[^\\w\\s]', '').str.split()\n",
    "\n",
    "# Aplanamanos la lista de palabras y eliminamos las stopwords\n",
    "stopwords_ingles = set(stopwords.words(\"english\"))\n",
    "todas_las_palabras = [palabra for titulo in titulos for palabra in titulo if palabra not in stopwords_ingles]\n",
    "\n",
    "# Contamos la frecuencia de palabras\n",
    "frecuencia_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Encontramos las palabras más comunes\n",
    "palabras_mas_comunes = frecuencia_palabras.most_common(20)\n",
    "\n",
    "print(\"Las 20 palabras más repetidas en los títulos del dataset\\\n",
    "\\nde artículos cintíficos de Arxiv sobre Inteligencia Artificial:\\\n",
    "\\n---------------------------------------------------------------\")\n",
    "# Imprimimos las palabras más comunes\n",
    "for palabra, cuenta in palabras_mas_comunes:\n",
    "    print(f'{palabra}: {cuenta}')\n",
    "\n",
    "# Registramos el tiempo de finalización\n",
    "tiempo_fin = time.time()\n",
    "\n",
    "# Calculamos la diferencia de tiempo\n",
    "tiempo_transcurrido = round((tiempo_fin - tiempo_inicio), 2)\n",
    "\n",
    "print(f\"---------------------------------------------------------------\\\n",
    "\\nTiempo de busqueda transcurrido: {tiempo_transcurrido} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desacernos de los vervos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 20 palabras más repetidas en los títulos del dataset\n",
      "de artículos cintíficos de Arxiv sobre Inteligencia Artificial:\n",
      "---------------------------------------------------------------\n",
      "design: 617\n",
      "neural: 411\n",
      "networks: 383\n",
      "network: 372\n",
      "deep: 371\n",
      "data: 310\n",
      "via: 310\n",
      "systems: 307\n",
      "control: 292\n",
      "detection: 256\n",
      "model: 240\n",
      "quantum: 237\n",
      "optimization: 232\n",
      "approach: 230\n",
      "graph: 206\n",
      "analysis: 205\n",
      "system: 202\n",
      "image: 195\n",
      "machine: 190\n",
      "framework: 185\n",
      "---------------------------------------------------------------\n",
      "Tiempo de busqueda transcurrido: 9.09 segundos\n"
     ]
    }
   ],
   "source": [
    "# Registramos tiempo de inicio\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# Preprocesamiento de Datos\n",
    "titulos = df_arxivs_papers['title'].str.lower().str.replace(r'[^\\w\\s]', '').str.split()\n",
    "\n",
    "# Aplanamos la lista de palabras y eliminamos las stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "todas_las_palabras = [palabra for titulo in titulos for palabra in titulo if palabra not in stop_words]\n",
    "\n",
    "# Filtramos los verbos\n",
    "palabras_filtradas = []\n",
    "for palabra in todas_las_palabras:\n",
    "    etiquetas_pos = pos_tag([palabra])\n",
    "    if etiquetas_pos[0][1] not in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Etiquetas de verbos\n",
    "        palabras_filtradas.append(palabra)\n",
    "\n",
    "# Contamos la frecuencia de palabras\n",
    "frecuencia_palabras = Counter(palabras_filtradas)\n",
    "\n",
    "# Encontramos las palabras más comunes\n",
    "palabras_mas_comunes = frecuencia_palabras.most_common(20)\n",
    "\n",
    "print(\"Las 20 palabras más repetidas en los títulos del dataset\\\n",
    "\\nde artículos cintíficos de Arxiv sobre Inteligencia Artificial:\\\n",
    "\\n---------------------------------------------------------------\")\n",
    "# Imprimimos las palabras más comunes\n",
    "for palabra, cuenta in palabras_mas_comunes:\n",
    "    print(f'{palabra}: {cuenta}')\n",
    "\n",
    "# Registramos el tiempo de finalización\n",
    "tiempo_fin = time.time()\n",
    "\n",
    "# Calculamos la diferencia de tiempo\n",
    "tiempo_transcurrido = round((tiempo_fin - tiempo_inicio), 2)\n",
    "\n",
    "print(f\"---------------------------------------------------------------\\\n",
    "\\nTiempo de busqueda transcurrido: {tiempo_transcurrido} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cuales son los bigramas más se repiten en los títulos. Un bigrama es una secuencia de dos palabras que aparecen juntas en un texto en un orden específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 20 bigramas más repetidos en los títulos de los artículos cintíficos del dataset de Arxiv sobre IA :\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "NOTA: Un bigrmama es una secuencia de dos palabras que aparecen juntas en un texto en un orden específico.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "neural networks: 154\n",
      "neural network: 112\n",
      "graph neural: 62\n",
      "object detection: 41\n",
      "convolutional neural: 41\n",
      "deep reinforcement: 39\n",
      "architecture search: 38\n",
      "semantic segmentation: 37\n",
      "deep neural: 36\n",
      "neural architecture: 33\n",
      "case study: 29\n",
      "massive mimo: 28\n",
      "generative adversarial: 27\n",
      "artificial intelligence: 27\n",
      "point cloud: 26\n",
      "domain adaptation: 22\n",
      "recommender systems: 21\n",
      "time series: 20\n",
      "medical image: 20\n",
      "image classification: 19\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Tiempo de busqueda transcurrido: 8.54 segundos\n"
     ]
    }
   ],
   "source": [
    "# Registramos el tiempo de inicio\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# Preprocesamiento de Datos\n",
    "titulos = df_arxivs_papers['title'].str.lower().str.replace(r'[^\\w\\s]', '').str.split()\n",
    "\n",
    "# Aplanamos la lista de palabras y eliminamos las stopwords\n",
    "stopwords_ingles = set(stopwords.words(\"english\"))\n",
    "todas_las_palabras = [palabra for titulo in titulos for palabra in titulo if palabra not in stopwords_ingles]\n",
    "\n",
    "# Filtramos los verbos\n",
    "palabras_filtradas = []\n",
    "for palabra in todas_las_palabras:\n",
    "    etiquetas_pos = pos_tag([palabra])\n",
    "    if etiquetas_pos[0][1] not in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Etiquetas de verbos\n",
    "        palabras_filtradas.append(palabra)\n",
    "\n",
    "# Creamos bigramas\n",
    "bigramas = list(ngrams(palabras_filtradas, 2))\n",
    "\n",
    "# Contamos la frecuencia de bigramas\n",
    "frecuencia_bigramas = Counter(bigramas)\n",
    "\n",
    "# Encontramos los bigramas más comunes\n",
    "bigramas_mas_comunes = frecuencia_bigramas.most_common(20)  # Cambia 10 al número deseado de bigramas\n",
    "\n",
    "print(\"Los 20 bigramas más repetidos en los títulos de los artículos cintíficos del dataset de Arxiv sobre IA :\\\n",
    "\\n---------------------------------------------------------------------------------------------------------\\\n",
    "\\nNOTA: Un bigrmama es una secuencia de dos palabras que aparecen juntas en un texto en un orden específico.\\\n",
    "\\n---------------------------------------------------------------------------------------------------------\")\n",
    "# Imprimimos los bigramas más comunes\n",
    "for bigrama, cuenta in bigramas_mas_comunes:\n",
    "    print(f'{bigrama[0]} {bigrama[1]}: {cuenta}')\n",
    "\n",
    "# Registramos el tiempo de finalización\n",
    "tiempo_fin = time.time()\n",
    "\n",
    "# Calculamos la diferencia de tiempo\n",
    "tiempo_transcurrido = round((tiempo_fin - tiempo_inicio), 2)\n",
    "\n",
    "print(f\"---------------------------------------------------------------------------------------------------------\\\n",
    "\\nTiempo de busqueda transcurrido: {tiempo_transcurrido} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busquemos ahora los trigrmas más se repiten en los títulos. Al igual que el bigrama, un trigrma es una secuencia, esta vez, de tres palabras que aparecen juntas en un texto en un orden específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 20 trigramas más repetidos en los títulos de los artículos cintíficos del dataset de Arxiv sobre IA :\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "NOTA: Un trigrama es una secuencia de tres palabras que aparecen juntas en un texto en un orden específico.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "graph neural networks: 45\n",
      "neural architecture search: 32\n",
      "convolutional neural network: 22\n",
      "deep neural networks: 18\n",
      "convolutional neural networks: 17\n",
      "deep neural network: 16\n",
      "massive mimo systems: 15\n",
      "graph neural network: 14\n",
      "medical image segmentation: 13\n",
      "model predictive control: 13\n",
      "generative adversarial network: 12\n",
      "generative adversarial networks: 12\n",
      "van der waals: 12\n",
      "unsupervised domain adaptation: 9\n",
      "quantum key distribution: 9\n",
      "neural networks via: 9\n",
      "salient object detection: 9\n",
      "graph convolutional networks: 8\n",
      "deep convolutional neural: 8\n",
      "cell-free massive mimo: 7\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Tiempo de busqueda transcurrido: 8.93 segundos\n"
     ]
    }
   ],
   "source": [
    "# Registrmos el tiempo de inicio\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# Preprocesamiento de Datos\n",
    "titulos = df_arxivs_papers['title'].str.lower().str.replace(r'[^\\w\\s]', '').str.split()\n",
    "\n",
    "# Aplananamo la lista de palabras y eliminamos las stopwords\n",
    "stopwords_ingles = set(stopwords.words(\"english\"))\n",
    "todas_las_palabras = [palabra for titulo in titulos for palabra in titulo if palabra not in stopwords_ingles]\n",
    "\n",
    "# Filtramos los verbos\n",
    "palabras_filtradas = []\n",
    "for palabra in todas_las_palabras:\n",
    "    etiquetas_pos = pos_tag([palabra])\n",
    "    if etiquetas_pos[0][1] not in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Etiquetas de verbos\n",
    "        palabras_filtradas.append(palabra)\n",
    "\n",
    "# Crear trigramas\n",
    "trigramas = list(ngrams(palabras_filtradas, 3))\n",
    "\n",
    "# Contamos la frecuencia de trigramas\n",
    "frecuencia_trigramas = Counter(trigramas)\n",
    "\n",
    "# Encontramos los trigramas más comunes\n",
    "trigramas_comunes = frecuencia_trigramas.most_common(20)  # Cambia 10 al número deseado de trigramas\n",
    "\n",
    "print(\"Los 20 trigramas más repetidos en los títulos de los artículos cintíficos del dataset de Arxiv sobre IA :\\\n",
    "\\n---------------------------------------------------------------------------------------------------------\\\n",
    "\\nNOTA: Un trigrama es una secuencia de tres palabras que aparecen juntas en un texto en un orden específico.\\\n",
    "\\n---------------------------------------------------------------------------------------------------------\")\n",
    "# Imprimimos los trigramas más comunes\n",
    "for trigrama, cuenta in trigramas_comunes:\n",
    "    print(f'{trigrama[0]} {trigrama[1]} {trigrama[2]}: {cuenta}')\n",
    "\n",
    "# Registramos el tiempo de finalización\n",
    "tiempo_fin = time.time()\n",
    "\n",
    "# Calculamos la diferencia de tiempo\n",
    "tiempo_transcurrido = round((tiempo_fin - tiempo_inicio), 2)\n",
    "\n",
    "print(f\"---------------------------------------------------------------------------------------------------------\\\n",
    "\\nTiempo de busqueda transcurrido: {tiempo_transcurrido} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 20 trigramas más repetidos en los abstracts de los artículos cintíficos del dataset de Arxiv sobre IA :\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "NOTA: Un trigrama es una secuencia de tres palabras que aparecen juntas en un texto en un orden específico.\n",
      "NOTA 2: Los abstracts contienen bastantes palabras más que los títulos de las publicaciones científicas.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "experimental results show: 154\n",
      "deep neural networks: 131\n",
      "deep neural network: 97\n",
      "convolutional neural network: 95\n",
      "paper, propose novel: 92\n",
      "convolutional neural networks: 88\n",
      "graph neural networks: 75\n",
      "neural architecture search: 75\n",
      "simulation results show: 71\n",
      "experimental results demonstrate: 68\n",
      "natural language processing: 63\n",
      "results show method: 47\n",
      "artificial intelligence (ai): 47\n",
      "graph neural network: 45\n",
      "extensive experiments demonstrate: 45\n",
      "paper, propose new: 43\n",
      "best knowledge, first: 43\n",
      "generative adversarial network: 43\n",
      "outperforms state-of-the-art methods: 42\n",
      "neural networks (gnns): 42\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Tiempo de busqueda transcurrido: 136.75 segundos\n"
     ]
    }
   ],
   "source": [
    "# Registrmos el tiempo de inicio\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# Preprocesamiento de Datos\n",
    "titulos = df_arxivs_papers['abstract'].str.lower().str.replace(r'[^\\w\\s]', '').str.split()\n",
    "\n",
    "# Aplananamo la lista de palabras y eliminamos las stopwords\n",
    "stopwords_ingles = set(stopwords.words(\"english\"))\n",
    "todas_las_palabras = [palabra for titulo in titulos for palabra in titulo if palabra not in stopwords_ingles]\n",
    "\n",
    "# Filtramos los verbos\n",
    "palabras_filtradas = []\n",
    "for palabra in todas_las_palabras:\n",
    "    etiquetas_pos = pos_tag([palabra])\n",
    "    if etiquetas_pos[0][1] not in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Etiquetas de verbos\n",
    "        palabras_filtradas.append(palabra)\n",
    "\n",
    "# Crear trigramas\n",
    "trigramas = list(ngrams(palabras_filtradas, 3))\n",
    "\n",
    "# Contamos la frecuencia de trigramas\n",
    "frecuencia_trigramas = Counter(trigramas)\n",
    "\n",
    "# Encontramos los trigramas más comunes\n",
    "trigramas_comunes = frecuencia_trigramas.most_common(20)  # Cambia 10 al número deseado de trigramas\n",
    "\n",
    "print(\"Los 20 trigramas más repetidos en los abstracts de los artículos cintíficos del dataset de Arxiv sobre IA :\\\n",
    "\\n---------------------------------------------------------------------------------------------------------\\\n",
    "\\nNOTA: Un trigrama es una secuencia de tres palabras que aparecen juntas en un texto en un orden específico.\\\n",
    "\\nNOTA 2: Los abstracts contienen bastantes palabras más que los títulos de las publicaciones científicas.\\\n",
    "\\n---------------------------------------------------------------------------------------------------------\")\n",
    "# Imprimimos los trigramas más comunes\n",
    "for trigrama, cuenta in trigramas_comunes:\n",
    "    print(f'{trigrama[0]} {trigrama[1]} {trigrama[2]}: {cuenta}')\n",
    "\n",
    "# Registramos el tiempo de finalización\n",
    "tiempo_fin = time.time()\n",
    "\n",
    "# Calculamos la diferencia de tiempo\n",
    "tiempo_transcurrido = round((tiempo_fin - tiempo_inicio), 2)\n",
    "\n",
    "print(f\"---------------------------------------------------------------------------------------------------------\\\n",
    "\\nTiempo de busqueda transcurrido: {tiempo_transcurrido} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el tiempo de busqueda de trigramas en los abstract de los artículos científico analizados es mucho más elevado que la empleada en la busqueda de trigramas realizada en los títulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *NOTA: En el futuro pasar los plurales a singular.*\n",
    "    \n",
    "    *De esta forma, por ejemplo \"graph neural networks\" que aparece 45 veces y \"graph neural network\" en 14 ocasiones, podrían sumarse y contar como un trigrama con 59 apariciones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conclusión: \n",
    "# Graph Neural Networks es el Futuro\n",
    "\n",
    "1. ***Graph Neural Networks*** (graph neural networks + graph neural network) es el trigrama más nombrado con un total de 59 apariciones. Le sigue en sunda posición ***Convolutional Neural Networks*** (convolutional neural networks + convolutional neural network) con 39 nombramientos. Y en tercera posición ***Deep Neural Networks*** (deep neural networks + deep neural network) hasta en 34 ocasiones. En cuarto lugar tendríamos a ***Generative Adversarial Netwoks*** (generative adversarial networks + generative adversarial network) con 24. Y finalmente, ***Masive MIMO Systems*** con 15.\n",
    "\n",
    "2. Observamos que en los abstract trigrama más utilizado es la ***Deep Neural Networks*** (deep neural networks + deep neural network) 228 veces. Después ***Convolutional Neural Networks*** (convolutional neural network + convolutional neural networks) 183 veces. El tercer trigrama es ***Experimerntal Results Shows***, 153 veces, que no nos aporta mucho, la verda. En cuarto lugar ***Graph Neural Networks*** (graph neural networks + graph neural network) con 120 menciones, sí bien unidos al trigrama ***Neural Networks (gnns)*** —GNN son las siglas Graph Neural Networks— el número ascendería a 160 mendiones, ascendiendio de este modo al podio en tercera posición."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
